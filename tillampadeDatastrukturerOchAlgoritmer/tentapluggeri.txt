Tillämpade datastrukturer och Algoritmer - Tentamen anteckningar
================================================================
ADT
    En mängd element och ett gäng operationer man kan utföra på dem jaooo
    Dictionary
        en ADT med operationerna add, remove och find
    Trees
        BST
            Ett sorterat träd där varje nod har max två barn.
            O(n) (Worst case)
            AVL träd
                Adelson-Velskii och Landis träd
                Ett BST där höjdskillnaden mellan en nodds (sic) barn är mindre än 2. Dvs. det
                är självbalanserande.
                O(logn) (worst case)
                Borttagning
                    hitta > ta bort > balansera > dansa (zumba)
            Traversering
                Pre order
                    root > left sub-tree > right sub-tree
                In order
                    left sub-tree > root > right sub-tree
                Post order
                    left sub-tree > right sub-tree > root
        B-Tree
            Bayer and McCreight 1972
            Can haz more than two the kidz
            Block structure
            O(logn)
            Commonly used in databases and filesystems
            Example of adding "3".
                Before
                        [7 16    ]
                    [1 2 5 6] [9 12    ] [18 21    ]
                After
                        [.7.16.    ]
                    [.1.2.3. ] [.5.6.  ] [.9.12.   ] [.18.21.  ]
                NOT like this, OK!?
                        [7 16    ]
                    [1 2 5 6] [9 12    ] [18 21    ]
                    [ 3     ]
        MST
            Minimum Spanning Tree
    Stack
        LIFO
            Last in first out
        pop()
        push()
        peek()
    Queue
        enqueue()
        dequeue()
        FIFO
            First in first out
        Priority Queue
    Heap
        "Queue tree"
        Array version
            Left child = n * 2 + 1
            Right child = n * 2 + 2
        Heap sort
            O(nlogn)
    Quick sort
        Divide and conquer
        eat hamburgerz
    Analysis
        The big O notation
    Graphs
        Nodes/edges and Arcs
        ADT for modelling relations
        Shortest Path
            DFS
                Depth first search
            BFS
                Breadth first search
            Algorithms
                Ford-Fulkerson
                    Checks for negative loops
                Dijkstra
                    Quicker, but needs graph without negative loops
    AI
        minmax
        alphabeta
        negamax

Appendix MST
============
Minimum spanning tree
From Wikipedia, the free encyclopedia
The minimum spanning tree of a planar graph. Each edge is labeled with its
weight, which here is roughly proportional to its length.
Given a connected, undirected graph, a spanning tree of that graph is a subgraph
that is a tree and connects all the vertices together. A single graph can have
many different spanning trees. We can also assign a weight to each edge, which
is a number representing how unfavorable it is, and use this to assign a weight
to a spanning tree by computing the sum of the weights of the edges in that
spanning tree. A minimum spanning tree (MST) or minimum weight spanning tree is
then a spanning tree with weight less than or equal to the weight of every other
spanning tree. More generally, any undirected graph (not necessarily connected)
has a minimum spanning forest, which is a union of minimum spanning trees for
its connected components.
One example would be a cable TV company laying cable to a new neighborhood. If
it is constrained to bury the cable only along certain paths, then there would
be a graph representing which points are connected by those paths. Some of those
paths might be more expensive, because they are longer, or require the cable to
be buried deeper; these paths would be represented by edges with larger weights.
A spanning tree for that graph would be a subset of those paths that has no
cycles but still connects to every house. There might be several spanning trees
possible. A minimum spanning tree would be one with the lowest total cost.
Contents [hide]
1 Properties
1.1 Possible multiplicity
1.2 Uniqueness
1.3 Minimum-cost subgraph
1.4 Cycle property
1.5 Cut property
1.6 Minimum-cost edge
2 Algorithms
3 MST on complete graphs
4 Related problems
5 Minimum bottleneck spanning tree
6 See also
7 References
8 Additional reading
9 External links
[edit]Properties

[edit]Possible multiplicity


This figure shows there may be more than one minimun spanning tree in a graph.
In the figure, the two trees below the graph are two possibilities of minimum
spanning tree of the given graph.
There may be several minimum spanning trees of the same weight having a minimum
number of edges; in particular, if all the edge weights of a given graph are the
same, then every spanning tree of that graph is minimum. If there are n vertices
in the graph, then each tree has n-1 edges.
[edit]Uniqueness
If each edge has a distinct weight then there will only be one, unique minimum
spanning tree. This can be proved by induction or contradiction. This is true in
many realistic situations, such as the cable TV company example above, where
it's unlikely any two paths have exactly the same cost. This generalizes to
spanning forests as well.
A proof of uniqueness by contradiction is as follows.[1]
Say we have an algorithm that finds an MST (which we will call A) based on the
structure of the graph and the order of the edges when ordered by weight. (Such
algorithms do exist, see below.)
Assume MST A is not unique.
There is another spanning tree with equal weight, say MST B.
Let e1 be an edge that is in A but not in B.
As B is a MST, {e1}  B must contain a cycle C.
Then B should include at least one edge e2 that is not in A and lies on C.
Assume the weight of e1 is less than that of e2.
Replace e2 with e1 in B yields the spanning tree {e1}  B - {e2} which has a
smaller weight compared to B.
Contradiction. As we assumed B is a MST but it is not.
If the weight of e1 is larger than that of e2, a similar argument involving tree
{e2}  A - {e1} also leads to a contradiction. Thus, we conclude that the
assumption that there can be a second MST was false.
[edit]Minimum-cost subgraph
If the weights are positive, then a minimum spanning tree is in fact the
minimum-cost subgraph connecting all vertices, since subgraphs containing cycles
necessarily have more total weight.
[edit]Cycle property
For any cycle C in the graph, if the weight of an edge e of C is larger than the
weights of other edges of C, then this edge cannot belong to an MST. Assuming
the contrary, i.e. that e belongs to an MST T1, then deleting e will break T1
into two subtrees with the two ends of e in different subtrees. The remainder of
C reconnects the subtrees, hence there is an edge f of C with ends in different
subtrees, i.e., it reconnects the subtrees into a tree T2 with weight less than
that of T1, because the weight of f is less than the weight of e.
[edit]Cut property


This figure shows the cut property of MSP.T is the MST of the given graph. If S
= {A,B,D,E}, thus V-S = {C,F},then there are 3 possibilities of the edge across
the cut(S,V-S), they are edges BC, EC, EF of the original graph. Then, e is one
of the minimum-weight-edge for the cut, therefore S ∪ {e} is part of the MST T.
For any cut C in the graph, if the weight of an edge e of C is smaller than the
weights of other edges of C, then this edge belongs to all MSTs of the graph.
Indeed, assume the contrary, for example, edge BC (weighted 6) belongs to the
MST T instead of edge e (weighted 4) in the left figure. Then adding e to T will
produce a cycle, while replacing BC with e would produce MST of smaller weight.
[edit]Minimum-cost edge
If the edge of a graph with the minimum cost e is unique, then this edge is
included in any MST. Indeed, if e was not included in the MST, removing any of
the (larger cost) edges in the cycle formed after adding e to the MST, would
yield a spanning tree of smaller weight.
[edit]Algorithms

The first algorithm for finding a minimum spanning tree was developed by Czech
scientist Otakar Borůvka in 1926 (see Borůvka's algorithm). Its purpose was an
efficient electrical coverage of Moravia. There are now two algorithms commonly
used, Prim's algorithm and Kruskal's algorithm. All three are greedy algorithms
that run in polynomial time, so the problem of finding such trees is in FP, and
related decision problems such as determining whether a particular edge is in
the MST or determining if the minimum total weight exceeds a certain value are
in P. Another greedy algorithm not as commonly used is the reverse-delete
algorithm, which is the reverse of Kruskal's algorithm.
If the edge weights are integers, then deterministic algorithms are known that
solve the problem in O(m + n) integer operations.[2] In a comparison model, in
which the only allowed operations on edge weights are pairwise comparisons,
Karger, Klein & Tarjan (1995) found a linear time randomized algorithm based on
a combination of Borůvka's algorithm and the reverse-delete algorithm.[3][4]
Whether the problem can be solved deterministically in linear time by a
comparison-based algorithm remains an open question, however. The fastest
non-randomized comparison-based algorithm, by Bernard Chazelle, is based on the
soft heap, an approximate priority queue.[5][6] Its running time is O(m α(m,n)),
where m is the number of edges, n is the number of vertices and α is the
classical functional inverse of the Ackermann function. The function α grows
extremely slowly, so that for all practical purposes it may be considered a
constant no greater than 4; thus Chazelle's algorithm takes very close to linear
time. Seth Pettie and Vijaya Ramachandran have found a provably optimal
deterministic comparison-based minimum spanning tree algorithm, the
computational complexity of which is unknown.[7]
Research has also considered parallel algorithms for the minimum spanning tree
problem. With a linear number of processors it is possible to solve the problem
in O(logn) time.[8][9] Bader & Cong (2003) demonstrate an algorithm that can
compute MSTs 5 times faster on 8 processors than an optimized sequential
algorithm.[10] Typically, parallel algorithms are based on Borůvka
algorithm—Prim's and especially Kruskal's algorithm do not scale as well to
additional processors.
Other specialized algorithms have been designed for computing minimum spanning
trees of a graph so large that most of it must be stored on disk at all times.
These external storage algorithms, for example as described in "Engineering an
External Memory Minimum Spanning Tree Algorithm" by Roman Dementiev et al.,[11]
can operate as little as 2 to 5 times slower than a traditional in-memory
algorithm; they claim that "massive minimum spanning tree problems filling
several hard disks can be solved overnight on a PC." They rely on efficient
external storage sorting algorithms and on graph contraction techniques for
reducing the graph's size efficiently.
The problem can also be approached in a distributed manner. If each node is
considered a computer and no node knows anything except its own connected links,
one can still calculate the distributed minimum spanning tree.
[edit]MST on complete graphs

Alan M. Frieze showed that given a complete graph on n vertices, with edge
weights that are independent identically distributed random variables with
distribution function F satisfying F'(0) > 0, then as n approaches +∞ the
expected weight of the MST approaches ζ(3) / F'(0), where ζ is the Riemann zeta
function. Under the additional assumption of finite variance, Alan M. Frieze
also proved convergence in probability. Subsequently, J. Michael Steele showed
that the variance assumption could be dropped.
In later work, Svante Janson proved a central limit theorem for weight of the
MST.
For uniform random weights in [0,1], the exact expected size of the minimum
spanning tree has been computed for small complete graphs.[12]
Vertices    Expected size   Approximative expected size
2   1 / 2   0.5
3   3 / 4   0.75
4   31 / 35 0.8857143
5   893 / 924   0.9664502
6   278 / 273   1.0183151
7   30739 / 29172   1.053716
8   199462271 / 184848378   1.0790588
9   126510063932 / 115228853025 1.0979027
[edit]Related problems

A related problem is the k-minimum spanning tree (k-MST), which is the tree that
spans some subset of k vertices in the graph with minimum weight.
A set of k-smallest spanning trees is a subset of k spanning trees (out of all
possible spanning trees) such that no spanning tree outside the subset has
smaller weight.[13][14][15] (Note that this problem is unrelated to the
k-minimum spanning tree.)
The Euclidean minimum spanning tree is a spanning tree of a graph with edge
weights corresponding to the Euclidean distance between vertices which are
points in the plane (or space).
The rectilinear minimum spanning tree is a spanning tree of a graph with edge
weights corresponding to the rectilinear distance between vertices which are
points in the plane (or space).
In the distributed model, where each node is considered a computer and no node
knows anything except its own connected links, one can consider distributed
minimum spanning tree. Mathematical definition of the problem is the same but
has different approaches for solution.
The capacitated minimum spanning tree is a tree that has a marked node (origin,
or root) and each of the subtrees attached to the node contains no more than a c
nodes. c is called a tree capacity. Solving CMST optimally requires exponential
time, but good heuristics such as Esau-Williams and Sharma produce solutions
close to optimal in polynomial time.
The degree constrained minimun spanning tree is a minimun spanning tree in with
each vertex is connected to no more than d other vertices, for some given number
d. The case d = 2 is a special case of the traveling salesman problem, so the
degree constrained minimum spanning tree is NP-hard in general.
For directed graphs, the minimum spanning tree problem is called the
Arborescence problem and can be solved in quadratic time using the
Chu–Liu/Edmonds algorithm.
A maximum spanning tree is a spanning tree with weight greater than or equal to
the weight of every other spanning tree. Such a tree can be found with
algorithms such as Prim's or Kruskal's after multiplying the edge weights by -1
and solving the MST problem on the new graph. A path in the maximum spanning
tree is the widest path in the graph between its two endpoints: among all
possible paths, it maximizes the weight of the minimum-weight edge.[16]
The dynamic MST problem concerns the update of a previously computed MST after
an edge weight change in the original graph or the insertion/deletion of a
vertex.[17][18]
[edit]Minimum bottleneck spanning tree

A bottleneck edge is the highest weighted edge in a spanning tree.
A spanning tree is a minimum bottleneck spanning tree (or MBST) if the graph
does not contain a spanning tree with a smaller bottleneck edge weight.
A MST is necessarily a MBST (provable by the cut property), but a MBST is not
necessarily a MST. If the bottleneck edge in a MBST is a bridge in the graph,
then all spanning trees are MBSTs.
[edit]

Appendix B-Tree
===============
From Wikipedia, the free encyclopedia
Not to be confused with Binary tree.
B-tree
Type    Tree
Invented    1972
Invented by Rudolf Bayer, Edward M. McCreight
Time complexity
in big O notation
Average Worst case
Space   O(n)    O(n)
Search  O(log n)    O(log n)
Insert  O(log n)    O(log n)
Delete  O(log n)    O(log n)
In computer science, a B-tree is a tree data structure that keeps data sorted
and allows searches, sequential access, insertions, and deletions in logarithmic
amortized time. The B-tree is a generalization of a binary search tree in that a
node can have more than two children. (Comer, p. 123) Unlike self-balancing
binary search trees, the B-tree is optimized for systems that read and write
large blocks of data. It is commonly used in databases and filesystems.


A B-tree of order 2 (Bayer & McCreight 1972) or order 5 (Knuth 1997).
Contents [hide]
1 Overview
1.1 Variants
1.2 Etymology unknown
2 The database problem
2.1 Time to search a sorted file
2.2 An index speeds the search
2.3 Insertions and deletions cause trouble
2.4 The B-tree uses all those ideas
3 Technical description
3.1 Terminology
3.2 Definition
4 Best case and worst case heights
5 Algorithms
5.1 Search
5.2 Insertion
5.3 Deletion
5.3.1 Deletion from a leaf node
5.3.2 Deletion from an internal node
5.3.3 Rebalancing after deletion
5.4 Initial construction
6 B-trees in filesystems
7 Variations
7.1 Access concurrency
8 See also
9 Notes
10 References
10.1 Original papers
11 External links
[edit]Overview

In B-trees, internal (non-leaf) nodes can have a variable number of child nodes
within some pre-defined range. When data is inserted or removed from a node, its
number of child nodes changes. In order to maintain the pre-defined range,
internal nodes may be joined or split. Because a range of child nodes is
permitted, B-trees do not need re-balancing as frequently as other
self-balancing search trees, but may waste some space, since nodes are not
entirely full. The lower and upper bounds on the number of child nodes are
typically fixed for a particular implementation. For example, in a 2-3 B-tree
(often simply referred to as a 2-3 tree), each internal node may have only 2 or
3 child nodes.
Each internal node of a B-tree will contain a number of keys. Usually, the
number of keys is chosen to vary between d and 2d. In practice, the keys take up
the most space in a node. The factor of 2 will guarantee that nodes can be split
or combined. If an internal node has 2d keys, then adding a key to that node can
be accomplished by splitting the 2d key node into two d key nodes and adding the
key to the parent node. Each split node has the required minimum number of keys.
Similarly, if an internal node and its neighbor each have d keys, then a key may
be deleted from the internal node by combining with its neighbor. Deleting the
key would make the internal node have d − 1 keys; joining the neighbor would add
d keys plus one more key brought down from the neighbor's parent. The result is
an entirely full node of 2d keys.
The number of branches (or child nodes) from a node will be one more than the
number of keys stored in the node. In a 2-3 B-tree, the internal nodes will
store either one key (with two child nodes) or two keys (with three child
nodes). A B-tree is sometimes described with the parameters (d + 1) — (2d + 1)
or simply with the highest branching order, (2d + 1).
A B-tree is kept balanced by requiring that all leaf nodes are at the same
depth. This depth will increase slowly as elements are added to the tree, but an
increase in the overall depth is infrequent, and results in all leaf nodes being
one more node further away from the root.
B-trees have substantial advantages over alternative implementations when node
access times far exceed access times within nodes. This usually occurs when the
nodes are in secondary storage such as disk drives. By maximizing the number of
child nodes within each internal node, the height of the tree decreases and the
number of expensive node accesses is reduced. In addition, rebalancing the tree
occurs less often. The maximum number of child nodes depends on the information
that must be stored for each child node and the size of a full disk block or an
analogous size in secondary storage. While 2-3 B-trees are easier to explain,
practical B-trees using secondary storage want a large number of child nodes to
improve performance.
[edit]Variants
The term B-tree may refer to a specific design or it may refer to a general
class of designs. In the narrow sense, a B-tree stores keys in its internal
nodes but need not store those keys in the records at the leaves. The general
class includes variations such as the B+-tree and the B*-tree.
In the B+-tree, copies of the keys are stored in the internal nodes; the keys
and records are stored in leaves; in addition, a leaf node may include a pointer
to the next leaf node to speed sequential access.(Comer, p. 129)
The B*-tree balances more neighboring internal nodes to keep the internal nodes
more densely packed.(Comer, p. 129) For example, a non-root node of a B-tree
must be at least half full, but a non-root node of a B*-tree must be at least
two-thirds full.
Counted B-trees store, with each pointer within the tree, the number of nodes in
the subtree below that pointer.[1] This allows rapid searches for the Nth record
in key order, or counting the number of records between any two records, and
various other related operations.
[edit]Etymology unknown
Rudolf Bayer and Ed McCreight invented the B-tree while working at Boeing
Research Labs in 1971 (Bayer & McCreight 1972), but they did not explain what,
if anything, the B stands for. Douglas Comer explains:
    The origin of "B-tree" has never been explained by the authors. As we shall
    see, "balanced," "broad," or "bushy" might apply. Others suggest that the
    "B" stands for Boeing. Because of his contributions, however, it seems
    appropriate to think of B-trees as "Bayer"-trees. (Comer 1979, p. 123
    footnote 1)
    Donald Knuth speculates on the etymology of B-trees in his May, 1980 lecture
    on the topic "CS144C classroom lecture about disk storage and B-trees",
    suggesting the "B" may have originated from Boeing or from Bayer's name.
    Knuth's video lectures from Stanford
    [edit]The database problem

    [edit]Time to search a sorted file
    Usually, sorting and searching algorithms have been characterized by the
    number of comparison operations that must be performed using order notation.
    A binary search of a sorted table with N records, for example, can be done
    in O(log2N) comparisons. If the table had 1,000,000 records, then a specific
    record could be located with about 20 comparisons: log21,000,000 =
    19.931....
    Large databases have historically been kept on disk drives. The time to read
    a record on a disk drive can dominate the time needed to compare keys once
    the record is available. The time to read a record from a disk drive
    involves a seek time and a rotational delay. The seek time may be 0 to 20 or
    more milliseconds, and the rotational delay averages about half the rotation
    period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For
    a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8
    milliseconds and the average reading seek time is 8.5 milliseconds.[2] For
    simplicity, assume reading from disk takes about 10 milliseconds.
    Naively, then, the time to locate one record out of a million would take 20
    disk reads times 10 milliseconds per disk read, which is 0.2 seconds.
    The time won't be that bad because individual records are grouped together
    in a disk block. A disk block might be 16 kilobytes. If each record is 160
    bytes, then 100 records could be stored in each block. The disk read time
    above was actually for an entire block. Once the disk head is in position,
    one or more disk blocks can be read with little delay. With 100 records per
    block, the last 6 or so comparisons don't need to do any disk reads—the
    comparisons are all within the last disk block read.
    To speed the search further, the first 13 to 14 comparisons (which each
    required a disk access) must be sped up.
    [edit]An index speeds the search
    A significant improvement can be made with an index. In the example above,
    initial disk reads narrowed the search range by a factor of two. That can be
    improved substantially by creating an auxiliary index that contains the
    first record in each disk block (sometimes called a sparse index). This
    auxiliary index would be 1% of the size of the original database, but it can
    be searched more quickly. Finding an entry in the auxiliary index would tell
    us which block to search in the main database; after searching the auxiliary
    index, we would have to search only that one block of the main database—at a
    cost of one more disk read. The index would hold 10,000 entries, so it would
    take at most 14 comparisons. Like the main database, the last 6 or so
    comparisons in the aux index would be on the same disk block. The index
    could be searched in about 8 disk reads, and the desired record could be
    accessed in 9 disk reads.
    The trick of creating an auxiliary index can be repeated to make an
    auxiliary index to the auxiliary index. That would make an aux-aux index
    that would need only 100 entries and would fit in one disk block.
    Instead of reading 14 disk blocks to find the desired record, we only need
    to read 3 blocks. Reading and searching the first (and only) block of the
    aux-aux index identifies the relevant block in aux-index. Reading and
    searching that aux-index block identifies the relevant block in the main
    database. Instead of 150 milliseconds, we need only 30 milliseconds to get
    the record.
    The auxiliary indices have turned the search problem from a binary search
    requiring roughly log2N disk reads to one requiring only logbN disk reads
    where b is the blocking factor (the number of entries per block: b = 100
    entries per block; logb1,000,000 = 3 reads).
    In practice, if the main database is being frequently searched, the aux-aux
    index and much of the aux index may reside in a disk cache, so they would
    not incur a disk read.
    [edit]Insertions and deletions cause trouble
    If the database does not change, then compiling the index is simple to do,
    and the index need never be changed. If there are changes, then managing the
    database and its index becomes more complicated.
    Deleting records from a database doesn't cause much trouble. The index can
    stay the same, and the record can just be marked as deleted. The database
    stays in sorted order. If there are a lot of deletions, then the searching
    and storage become less efficient.
    Insertions are a disaster in a sorted sequential file because room for the
    inserted record must be made. Inserting a record before the first record in
    the file requires shifting all of the records down one. Such an operation is
    just too expensive to be practical.
    A trick is to leave some space lying around to be used for insertions.
    Instead of densely storing all the records in a block, the block can have
    some free space to allow for subsequent insertions. Those records would be
    marked as if they were "deleted" records.
    Now, both insertions and deletions are fast as long as space is available on
    a block. If an insertion won't fit on the block, then some free space on
    some nearby block must be found and the auxiliary indices adjusted. The hope
    is enough space is nearby that a lot of blocks do not need to be
    reorganized. Alternatively, some out-of-sequence disk blocks may be used.
    [edit]The B-tree uses all those ideas
    The B-tree uses all the above ideas. It keeps the records in sorted order so
    they may be sequentially traversed. It uses a hierarchical index to minimize
    the number of disk reads. The index is elegantly adjusted with a recursive
    algorithm. The B-tree uses partially full blocks to speed insertions and
    deletions. In addition, a B-tree minimizes waste by making sure the interior
    nodes are at least 1/2 full. A B-tree can handle an arbitrary number of
    insertions and deletions.
    [edit]Technical description

    [edit]Terminology
    The terminology used for B-trees is inconsistent in the literature:
    Unfortunately, the literature on B-trees is not uniform in its use of terms
    relating to B-Trees. (Folk & Zoellick 1992, p. 362)
    Bayer & McCreight (1972), Comer (1979), and others define the order of
    B-tree as the minimum number of keys in a non-root node. Folk & Zoellick
    (1992) points out that terminology is ambiguous because the maximum number
    of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a
    maximum of 7 keys. Knuth (1993b) avoids the problem by defining the order to
    be maximum number of children (which is one more than the maximum number of
    keys).
    The term leaf is also inconsistent. Bayer & McCreight (1972) considered the
    leaf level to be the lowest level of keys, but Knuth (1993b) considered the
    leaf level to be one level below the lowest keys. (Folk & Zoellick 1992, p.
    363) There are many possible implementation choices. In some designs, the
    leaves may hold the entire data record; in other designs, the leaves may
    only hold pointers to the data record. Those choices are not fundamental to
    the idea of a B-tree.[3]
    There are also unfortunate choices like using the variable k to represent
    the number of children when k could be confused with the number of keys.
    For simplicity, most authors assume there are a fixed number of keys that
    fit in a node. The basic assumption is the key size is fixed and the node
    size is fixed. In practice, variable length keys may be employed. (Folk &
    Zoellick 1992, p. 379)
    [edit]Definition
    According to Knuth's definition, a B-tree of order m (the maximum number of
    children for each node) is a tree which satisfies the following properties:
    Every node has at most m children.
    Every node (except root) has at least m⁄2 children.
    The root has at least two children if it is not a leaf node.
    All leaves appear in the same level, and carry information.
    A non-leaf node with k children contains k−1 keys.
    Each internal node's elements act as separation values which divide its
    subtrees. For example, if an internal node has three child nodes (or
    subtrees) then it must have two separation values or elements a1 and a2. All
    values in the leftmost subtree will be less than a1 , all values in the
    middle subtree will be between a1 and a2, and all values in the rightmost
    subtree will be greater than a2.
    Internal nodes in a B-tree – nodes which are not leaf nodes – are usually
    represented as an ordered set of elements and child pointers. Every internal
    node contains a maximum of U children and – other than the root – a minimum
    of L children. For all internal nodes other than the root, the number of
    elements is one less than the number of child pointers; the number of
    elements is between L−1 and U−1. The number U must be either 2L or 2L−1;
    thus each internal node is at least half full. This relationship between U
    and L implies that two half-full nodes can be joined to make a legal node,
    and one full node can be split into two legal nodes (if there is room to
    push one element up into the parent). These properties make it possible to
    delete and insert new values into a B-tree and adjust the tree to preserve
    the B-tree properties.
    Leaf nodes have the same restriction on the number of elements, but have no
    children, and no child pointers.
    The root node still has the upper limit on the number of children, but has
    no lower limit. For example, when there are fewer than L−1 elements in the
    entire tree, the root will be the only node in the tree, and it will have no
    children at all.
    A B-tree of depth n+1 can hold about U times as many items as a B-tree of
    depth n, but the cost of search, insert, and delete operations grows with
    the depth of the tree. As with any balanced tree, the cost grows much more
    slowly than the number of elements.
    Some balanced trees store values only at the leaf nodes, and so have
    different kinds of nodes for leaf nodes and internal nodes. B-trees keep
    values in every node in the tree, and may use the same structure for all
    nodes. However, since leaf nodes never have children, a specialized
    structure for leaf nodes in B-trees will improve performance.
    [edit]Best case and worst case heights

    The best case height of a B-Tree is:

    The worst case height of a B-Tree is:

    where m is the maximum number of children a node can have.
    [edit]Algorithms

    Warning: the discussion below uses "element", "value", "key", "separator",
    and "separation value" to mean essentially the same thing. The terms are not
    clearly defined. There are some subtle issues at the root and leaves.
    [edit]Search
    Searching is similar to searching a binary search tree. Starting at the
    root, the tree is recursively traversed from top to bottom. At each level,
    the search chooses the child pointer (subtree) whose separation values are
    on either side of the search value.
    Binary search is typically (but not necessarily) used within nodes to find
    the separation values and child tree of interest.
    [edit]Insertion


    A B Tree insertion example with each iteration.
    All insertions start at a leaf node. To insert a new element
    Search the tree to find the leaf node where the new element should be added.
    Insert the new element into that node with the following steps:
    If the node contains fewer than the maximum legal number of elements, then
    there is room for the new element. Insert the new element in the node,
    keeping the node's elements ordered.
    Otherwise the node is full, so evenly split it into two nodes.
    A single median is chosen from among the leaf's elements and the new
    element.
    Values less than the median are put in the new left node and values greater
    than the median are put in the new right node, with the median acting as a
    separation value.
    Insert the separation value in the node's parent, which may cause it to be
    split, and so on. If the node has no parent (i.e., the node was the root),
    create a new root above this node (increasing the height of the tree).
    If the splitting goes all the way up to the root, it creates a new root with
    a single separator value and two children, which is why the lower bound on
    the size of internal nodes does not apply to the root. The maximum number of
    elements per node is U−1. When a node is split, one element moves to the
    parent, but one element is added. So, it must be possible to divide the
    maximum number U−1 of elements into two legal nodes. If this number is odd,
    then U=2L and one of the new nodes contains (U−2)/2 = L−1 elements, and
    hence is a legal node, and the other contains one more element, and hence it
    is legal too. If U−1 is even, then U=2L−1, so there are 2L−2 elements in the
    node. Half of this number is L−1, which is the minimum number of elements
    allowed per node.
    An improved algorithm supports a single pass down the tree from the root to
    the node where the insertion will take place, splitting any full nodes
    encountered on the way. This prevents the need to recall the parent nodes
    into memory, which may be expensive if the nodes are on secondary storage.
    However, to use this improved algorithm, we must be able to send one element
    to the parent and split the remaining U−2 elements into two legal nodes,
    without adding a new element. This requires U = 2L rather than U = 2L−1,
    which accounts for why some textbooks impose this requirement in defining
    B-trees.
    [edit]Deletion
    There are two popular strategies for deletion from a B-Tree.
    locate and delete the item, then restructure the tree to regain its
    invariants
    or
    do a single pass down the tree, but before entering (visiting) a node,
        restructure the tree so that once the key to be deleted is encountered,
        it can be deleted without triggering the need for any further
        restructuring
        The algorithm below uses the former strategy.
        There are two special cases to consider when deleting an element:
        the element in an internal node may be a separator for its child nodes
        deleting an element may put its node under the minimum number of
        elements and children.
        Each of these cases will be dealt with in order.
        [edit]Deletion from a leaf node
        Search for the value to delete.
        If the value is in a leaf node, it can simply be deleted from the node,
        If underflow happens, check siblings to either transfer a key or fuse
        the siblings together.
        if deletion happened from right child retrieve the max value of left
            child if there is no underflow in left child
            in vice-versa situation retrieve the min element from right
            [edit]Deletion from an internal node
            Each element in an internal node acts as a separation value for two
            subtrees, and when such an element is deleted, two cases arise. In
            the first case, both of the two child nodes to the left and right of
            the deleted element have the minimum number of elements, namely L−1.
            They can then be joined into a single node with 2L−2 elements, a
            number which does not exceed U−1 and so is a legal node. Unless it
            is known that this particular B-tree does not contain duplicate
            data, we must then also (recursively) delete the element in question
            from the new node.
            In the second case, one of the two child nodes contains more than
            the minimum number of elements. Then a new separator for those
            subtrees must be found. Note that the largest element in the left
            subtree is still less than the separator. Likewise, the smallest
            element in the right subtree is the smallest element which is still
            greater than the separator. Both of those elements are in leaf
            nodes, and either can be the new separator for the two subtrees.
            If the value is in an internal node, choose a new separator (either
            the largest element in the left subtree or the smallest element in
            the right subtree), remove it from the leaf node it is in, and
            replace the element to be deleted with the new separator.
            This has deleted an element from a leaf node, and so is now
            equivalent to the previous case.
            [edit]Rebalancing after deletion
            If deleting an element from a leaf node has brought it under the
            minimum size, some elements must be redistributed to bring all nodes
            up to the minimum. In some cases the rearrangement will move the
            deficiency to the parent, and the redistribution must be applied
            iteratively up the tree, perhaps even to the root. Since the minimum
            element count doesn't apply to the root, making the root be the only
            deficient node is not a problem. The algorithm to rebalance the tree
            is as follows:[citation needed]
            If the right sibling has more than the minimum number of elements
            Add the separator to the end of the deficient node.
            Replace the separator in the parent with the first element of the
            right sibling.
            Append the first child of the right sibling as the last child of the
            deficient node
            Otherwise, if the left sibling has more than the minimum number of
            elements.
            Add the separator to the start of the deficient node.
            Replace the separator in the parent with the last element of the
            left sibling.
            Insert the last child of the left sibling as the first child of the
            deficient node
            If both immediate siblings have only the minimum number of elements
            Create a new node with all the elements from the deficient node, all
            the elements from one of its siblings, and the separator in the
            parent between the two combined sibling nodes.
            Remove the separator from the parent, and replace the two children
            it separated with the combined node.
            If that brings the number of elements in the parent under the
            minimum, repeat these steps with that deficient node, unless it is
            the root, since the root is permitted to be deficient.
            The only other case to account for is when the root has no elements
            and one child. In this case it is sufficient to replace it with its
            only child.
            [edit]Initial construction
            In applications, it is frequently useful to build a B-tree to
            represent a large existing collection of data and then update it
            incrementally using standard B-tree operations. In this case, the
            most efficient way to construct the initial B-tree is not to insert
            every element in the initial collection successively, but instead to
            construct the initial set of leaf nodes directly from the input,
            then build the internal nodes from these. This approach to B-tree
            construction is called bulkloading. Initially, every leaf but the
            last one has one extra element, which will be used to build the
            internal nodes.[citation needed]
            For example, if the leaf nodes have maximum size 4 and the initial
            collection is the integers 1 through 24, we would initially
            construct 4 leaf nodes containing 5 values each and 1 which contains
            4 values:
            1   2   3   4   5
            6   7   8   9   10
            11  12  13  14  15
            16  17  18  19  20
            21  22  23  24
            We build the next level up from the leaves by taking the last
            element from each leaf node except the last one. Again, each node
            except the last will contain one extra value. In the example,
            suppose the internal nodes contain at most 2 values (3 child
            pointers). Then the next level up of internal nodes would be:
            5   10  15
            20
            1   2   3   4
            6   7   8   9
            11  12  13  14
            16  17  18  19
            21  22  23  24
            This process is continued until we reach a level with only one node
            and it is not overfilled. In the example only the root level
            remains:
            15
            5   10
            20
            1   2   3   4
            6   7   8   9
            11  12  13  14
            16  17  18  19
            21  22  23  24
            [edit]B-trees in filesystems

            In addition to its use in databases, the B-tree is also used in
            filesystems to allow quick random access to an arbitrary block in a
            particular file. The basic problem is turning the file block i
            address into a disk block (or perhaps to a cylinder-head-sector)
            address.
            Some operating systems require the user to allocate the maximum size
            of the file when the file is created. The file can then be allocated
            as contiguous disk blocks. Converting to a disk block: the operating
            system just adds the file block address to the starting disk block
            of the file. The scheme is simple, but the file cannot exceed its
            created size.
            Other operating systems allow a file to grow. The resulting disk
            blocks may not be contiguous, so mapping logical blocks to physical
            blocks is more involved.
            MS-DOS, for example, used a simple File Allocation Table (FAT). The
            FAT has an entry for each disk block,[note 1] and that entry
            identifies whether its block is used by a file and if so, which
            block (if any) is the next disk block of the same file. So, the
            allocation of each file is represented as a linked list in the
            table. In order to find the disk address of file block i, the
            operating system (or disk utility) must sequentially follow the
            file's linked list in the FAT. Worse, to find a free disk block, it
            must sequentially scan the FAT. For MS-DOS, that was not a huge
            penalty because the disks and files were small and the FAT had few
            entries and relatively short file chains. In the FAT12 filesystem
            (used on floppy disks and early hard disks), there were no more than
            4,080 [note 2] entries, and the FAT would usually be resident in
            memory. As disks got bigger, the FAT architecture began to confront
            penalties. On a large disk using FAT, it may be necessary to perform
            disk reads to learn the disk location of a file block to be read or
            written.
            TOPS-20 (and possibly TENEX) used a 0 to 2 level tree that has
            similarities to a B-Tree[citation needed]. A disk block was 512
            36-bit words. If the file fit in a 512 (29) word block, then the
            file directory would point to that physical disk block. If the file
            fit in 218 words, then the directory would point to an aux index;
            the 512 words of that index would either be NULL (the block isn't
            allocated) or point to the physical address of the block. If the
            file fit in 227 words, then the directory would point to a block
            holding an aux-aux index; each entry would either be NULL or point
            to an aux index. Consequently, the physical disk block for a 227
            word file could be located in two disk reads and read on the third.
            Apple's filesystem HFS+, Microsoft's NTFS[4] and some Linux
            filesystems, such as btrfs and Ext4, use B-trees.
            [edit]Variations

            [edit]Access concurrency
            Lehman and Yao[5] showed that all read locks could be avoided (and
            thus concurrent access greatly improved) by linking the tree blocks
            at each level together with a "next" pointer. This results in a tree
            structure where both insertion and search operations descend from
            the root to the leaf. Write locks are only required as a tree block
            is modified. This maximizes access concurrency by multiple users, an
            important consideration for databases and/or other B-Tree based ISAM
            storage methods. The cost associated with this improvement is that
            empty pages cannot be removed from the btree during normal
            operations. (However, see [6] for various strategies to implement
            node merging, and source code at.[7])
            United States Patent 5283894, granted In 1994, appears to show a way
            to use a 'Meta Access Method' [8] to allow concurrent B+Tree access
            and modification without locks. The technique accesses the tree
            'upwards' for both searches and updates by means of additional
            in-memory indexes that point at the blocks in each level in the
            block cache. No reorganization for deletes is needed and there are
            no 'next' pointers in each block as in Lehman and Yao.

Appendix Heap
=============
Heap (data structure)
From Wikipedia, the free encyclopedia
This article is about the programming data structure. For the dynamic memory
area, see Dynamic memory allocation.


Example of a complete binary max-heap
In computer science, a heap is a specialized tree-based data structure that
satisfies the heap property: if B is a child node of A, then key(A) ≥ key(B).
This implies that an element with the greatest key is always in the root node,
and so such a heap is sometimes called a max-heap. (Alternatively, if the
comparison is reversed, the smallest element is always in the root node, which
results in a min-heap.) There is no restriction as to how many children each
node has in a heap, although in practice each node has at most two. The heap is
one maximally-efficient implementation of an abstract data type called a
priority queue. Heaps are crucial in several efficient graph algorithms such as
Dijkstra's algorithm, and in the sorting algorithm heapsort.
A heap data structure should not be confused with the heap which is a common
name for dynamically allocated memory. The term was originally used only for the
data structure. Some early popular languages such as LISP provided dynamic
memory allocation using heap data structures, which gave the memory area its
name[1].
Heaps are usually implemented in an array, and do not require pointers between
elements.
The operations commonly performed with a heap are:
find-max or find-min: find the maximum item of a max-heap or a minimum item of a
min-heap, respectively
delete-max or delete-min: removing the root node of a max- or min-heap,
respectively
increase-key or decrease-key: updating a key within a max- or min-heap,
respectively
insert: adding a new key to the heap
merge: joining two heaps to form a valid new heap containing all the elements of
both.
Contents [hide]
1 Variants
2 Comparison of theoretic bounds for variants
3 Applications
4 Implementations
5 See also
6 References
[edit]Variants

2-3 heap
Beap
Binary heap
Binomial heap
Brodal queue
D-ary heap
Fibonacci heap
Leftist heap
Pairing heap
Skew heap
Soft heap
Ternary heap
Treap
[edit]Comparison of theoretic bounds for variants

The following time complexities[1] are amortized (worst-time) time complexity
for entries marked by an asterisk, and regular worst case time complexities for
    all other entries. O(f) gives asymptotic upper bound and Θ(f) is
    asymptotically tight bound (see Big O notation). Function names assume a
    min-heap.
    Operation   Binary  Binomial    Fibonacci   Pairing[2]  Brodal
    findMin Θ(1)    Θ(log n) or Θ(1)    Θ(1)[1] Θ(1)[citation needed]
    Θ(1)[citation needed]
    deleteMin   Θ(log n)    Θ(log n)    O(log n)*   O(log n)*   O(log n)
    insert  Θ(log n)    O(log n)    Θ(1)[citation needed]   O(1)*[citation
    needed] Θ(1)[citation needed]
    decreaseKey Θ(log n)    Θ(log n)    Θ(1)*   O(log n)*   Θ(1)
    merge   Θ(n)    O(log n)**  Θ(1)    O(1)*   Θ(1)
    (*)Amortized time
    (**)Where n is the size of the larger heap
    Note that a "Brodal queue" is an implementation of a parallel priority queue
    created by Gerth Stølting Brodal et al.[3]
    [edit]Applications

    The heap data structure has many applications.
    Heapsort: One of the best sorting methods being in-place and with no
    quadratic worst-case scenarios.
    Selection algorithms: Finding the min, max, both the min and max, median, or
    even the k-th largest element can be done in linear time (often constant
    time) using heaps.[4]
    Graph algorithms: By using heaps as internal traversal data structures, run
    time will be reduced by polynomial order. Examples of such problems are
    Prim's minimal spanning tree algorithm and Dijkstra's shortest path problem.
    Full and almost full binary heaps may be represented in a very
    space-efficient way using an array alone. The first (or last) element will
    contain the root. The next two elements of the array contain its children.
    The next four contain the four children of the two child nodes, etc. Thus
    the children of the node at position n would be at positions 2n and 2n+1 in
    a one-based array, or 2n+1 and 2n+2 in a zero-based array. This allows
    moving up or down the tree by doing simple index computations. Balancing a
    heap is done by swapping elements which are out of order. As we can build a
    heap from an array without requiring extra memory (for the nodes, for
    example), heapsort can be used to sort an array in-place.
    One more advantage of heaps over trees in some applications is that
    construction of heaps can be done in linear time using Tarjan's
    algorithm.[clarification needed]
    [edit]Implementations

    The C++ Standard Template Library provides the make_heap, push_heap and
    pop_heap algorithms for heaps (usually implemented as binary heaps), which
    operate on arbitrary random access iterators. It treats the iterators as a
    reference to an array, and uses the array-to-heap conversion.
    The Java 2 platform (since version 1.5) provides the binary heap
    implementation with class java.util.PriorityQueue<E> in Java Collections
    Framework.
    Python has a heapq module that implements a priority queue using a binary
    heap.
    PHP has both maxheap (SplMaxHeap) and minheap (SplMinHeap) as of version 5.3
    in the Standard PHP Library.
    Perl has implementations of binary, binomial, and Fibonacci heaps in the
    Heap distribution available on CPAN.
    

Quicksort
=========
Visualization of the quicksort algorithm. The horizontal lines are pivot values.
Class	Sorting algorithm
Worst case performance	O(n2)
Best case performance	O(n log n)
Average case performance	O(n log n)
Worst case space complexity	O(n)
Quicksort is a sorting algorithm developed by Tony Hoare that, on average, makes O(nlogn) (big O notation) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is usually rare. Quicksort is often faster in practice than other O(nlogn) algorithms[citation needed]. Additionally, quicksort's sequential and localized memory references work well with a cache. Quicksort can be implemented as an in-place sort, requiring only O(logn) additional space.[1]
Quicksort (also known as "partition-exchange sort") is a comparison sort and, in space efficient implementations, is not a stable sort.
Contents [hide]
1 History
2 Algorithm
2.1 Simple version
2.2 In-place version
2.3 Implementation issues
2.3.1 Choice of pivot
2.3.2 Optimizations
2.3.3 Parallelization
3 Formal analysis
3.1 Randomized quicksort expected complexity
3.2 Average complexity
3.3 Space complexity
4 Selection-based pivoting
5 Variants
6 Comparison with other sorting algorithms
7 See also
8 Notes
9 References
10 External links
[edit]History

The quicksort algorithm was developed in 1960 by Tony Hoare while in the Soviet Union, as a visiting student at Moscow State University. At that time, Hoare worked in a project on machine translation for the National Physical Laboratory. He developed the algorithm in order to sort the words to be translated, to make them more easily matched to an already-sorted Russian-to-English dictionary that was stored on magnetic tape.[2]
[edit]Algorithm



Full example of quicksort on a random set of numbers. The shaded element is the pivot. It is always chosen as the last element of the partition. However, always choosing the last element in the partition as the pivot in this way results in poor performance (O(n2)) on already sorted lists, or lists of identical elements. Since sub-lists of sorted / identical elements crop up a lot towards the end of a sorting procedure on a large set, versions of the quicksort algorithm which choose the pivot as the middle element run much more quickly than the algorithm described in this diagram on large sets of numbers.
Quicksort is a divide and conquer algorithm. Quicksort first divides a large list into two smaller sub-lists: the low elements and the high elements. Quicksort can then recursively sort the sub-lists.
The steps are:
Pick an element, called a pivot, from the list.
Reorder the list so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.
Recursively sort the sub-list of lesser elements and the sub-list of greater elements.
The base case of the recursion are lists of size zero or one, which never need to be sorted.
[edit]Simple version
In simple pseudocode, the algorithm might be expressed as this:
 function quicksort(array)
     var list less, greater
     if length(array) ≤ 1
         return array  // an array of zero or one elements is already sorted
     select and remove a pivot value pivot from array
     for each x in array
         if x ≤ pivot then append x to less
         else append x to greater
     return concatenate(quicksort(less), pivot, quicksort(greater))
Notice that we only examine elements by comparing them to other elements. This makes quicksort a comparison sort. This version is also a stable sort (assuming that the "for each" method retrieves elements in original order, and the pivot selected is the last among those of equal value).
The correctness of the partition algorithm is based on the following two arguments:
At each iteration, all the elements processed so far are in the desired position: before the pivot if less than the pivot's value, after the pivot if greater than the pivot's value (loop invariant).
Each iteration leaves one fewer element to be processed (loop variant).
The correctness of the overall algorithm can be proven via induction: for zero or one element, the algorithm leaves the data unchanged; for a larger data set it produces the concatenation of two parts, elements less than the pivot and elements greater than it, themselves sorted by the recursive hypothesis.


An example on quicksort.
[edit]In-place version
The disadvantage of the simple version above is that it requires O(n) extra storage space, which is as bad as merge sort. The additional memory allocations required can also drastically impact speed and cache performance in practical implementations. There is a more complex version which uses an in-place partition algorithm and can achieve the complete sort using O(log n) space (not counting the input) use on average (for the call stack):
  // left is the index of the leftmost element of the array
  // right is the index of the rightmost element of the array (inclusive)
  //   number of elements in subarray: right-left+1
  function partition(array, left, right, pivotIndex)
     pivotValue := array[pivotIndex]
     swap array[pivotIndex] and array[right]  // Move pivot to end
     storeIndex := left
     for i  from  left to right - 1 // left ≤ i < right
         if array[i] < pivotValue
             swap array[i] and array[storeIndex]
             storeIndex := storeIndex + 1
     swap array[storeIndex] and array[right]  // Move pivot to its final place
     return storeIndex


In-place partition in action on a small list. The boxed element is the pivot element, blue elements are less or equal, and red elements are larger.
This is the in-place partition algorithm. It partitions the portion of the array between indexes left and right, inclusively, by moving all elements less than or equal to array[pivotIndex] to the beginning of the subarray, leaving all the greater elements following them. In the process it also finds the final position for the pivot element, which it returns. It temporarily moves the pivot element to the end of the subarray, so that it doesn't get in the way. Because it only uses exchanges, the final list has the same elements as the original list. Notice that an element may be exchanged multiple times before reaching its final place. Also, in case of pivot duplicates in the input array, they can be spread across left subarray, possibly in random order. This doesn't represent a partitioning failure, as further sorting will reposition and finally "glue" them together.
This form of the partition algorithm is not the original form; multiple variations can be found in various textbooks, such as versions not having the storeIndex. However, this form is probably the easiest to understand.
Once we have this, writing quicksort itself is easy:
 function quicksort(array, left, right)
     if right > left  // subarray of 0 or 1 elements already sorted
         select a pivotIndex in the range left ≤ pivotIndex ≤ right  // see Choice of pivot for possible choices
         pivotNewIndex := partition(array, left, right, pivotIndex)  // element at pivotNewIndex is now at its final position
         quicksort(array, left, pivotNewIndex - 1)  // recursively sort elements on the left of pivotNewIndex
         quicksort(array, pivotNewIndex + 1, right)  // recursively sort elements on the right of pivotNewIndex
Each recursive call to this quicksort function reduces the size of the array being sorted by at least one element, since in each invocation the element at pivotNewIndex is placed in its final position. Therefore, this algorithm is guaranteed to terminate after at most n recursive calls. However, since partition reorders elements within a partition, this version of quicksort is not a stable sort.
[edit]Implementation issues
[edit]Choice of pivot
In very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by R. Sedgewick).[3][4]
Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the naive expression for the middle index, (left + right)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, left + (right-left)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.
[edit]Optimizations
Two other important optimizations, also suggested by R. Sedgewick, as commonly acknowledged, and widely used in practice[5][6][7] are:
To make sure at most O(log N) space is used, recurse first into the smaller half of the array, and use a tail call to recurse into the other.
Use insertion sort, which has a smaller constant factor and is thus faster on small arrays, for invocations on such small arrays (i.e. where the length is less than a threshold t determined experimentally). This can be implemented by leaving such arrays unsorted and running a single insertion sort pass at the end, because insertion sort handles nearly sorted arrays efficiently. A separate insertion sort of each small segment as they are identified adds the overhead of starting and stopping many small sorts, but, avoids wasting effort comparing keys across the many segment boundaries, which keys will be in order due to the workings of the quicksort process. It also improves the cache use.
[edit]Parallelization
Like merge sort, quicksort can also be easily parallelized due to its divide-and-conquer nature. Individual in-place partition operations are difficult to parallelize, but once divided, different sections of the list can be sorted in parallel. If we have p processors, we can divide a list of n elements into p sublists in  average time, then sort each of these in average time. Ignoring the  preprocessing and the time required to merge the sorted sublists, this is linear speedup. Given n processors, only  time is required overall.
One advantage of parallel quicksort over other parallel sort algorithms is that no synchronization is required. A new thread is started as soon as a sublist is available for it to work on and it does not communicate with other threads. When all threads complete, the sort is done.
Other more sophisticated parallel sorting algorithms can achieve even better time bounds.[8] For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in  time given enough processors by performing partitioning implicitly.[9]
[edit]Formal analysis

From the initial description it's not obvious that quicksort takes  time on average. It's not hard to see that the partition operation, which simply loops over the elements of the array once, uses  time. In versions that perform concatenation, this operation is also .
In the best case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only logn nested calls before we reach a list of size 1. This means that the depth of the call tree is . But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only  time all together (each call has some constant overhead, but since there are only  calls at each level, this is subsumed in the  factor). The result is that the algorithm uses only  time.
An alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. Because a single quicksort call involves  factor work plus two recursive calls on lists of size n / 2 in the best case, the relation would be:

The master theorem tells us that .
In fact, it's not necessary to divide the list this precisely; even if each pivot splits the elements with 99% on one side and 1% on the other (or any other fixed fraction), the call depth is still limited to 100logn, so the total running time is still .
In the worst case, however, the two sublists have size 1 and n − 1 (for example, if the array consists of the same element by value), and the call tree becomes a linear chain of n nested calls. The ith call does  work, and . The recurrence relation is:

This is the same relation as for insertion sort and selection sort, and it solves to . Given knowledge of which comparisons are performed by the sort, there are adaptive algorithms that are effective at generating worst-case input for quicksort on-the-fly, regardless of the pivot selection strategy.[10]
[edit]Randomized quicksort expected complexity
Randomized quicksort has the desirable property that, for any input, it requires only  expected time (averaged over all choices of pivots). But what makes random pivots a good choice?
Suppose we sort the list and then divide it into four parts. The two parts in the middle will contain the best pivots; each of them is larger than at least 25% of the elements and smaller than at least 25% of the elements. If we could consistently choose an element from these two middle parts, we would only have to split the list at most 2log2n times before reaching lists of size 1, yielding an  algorithm.
A random choice will only choose from these middle parts half the time. However, this is good enough. Imagine that you are flipping a coin over and over until you get k heads. Although this could take a long time, on average only 2k flips are required, and the chance that you won't get k heads after 100k flips is highly improbable. By the same argument, quicksort's recursion will terminate on average at a call depth of only 2(2log2n). But if its average call depth is , and each level of the call tree processes at most n elements, the total amount of work done on average is the product, . Note that the algorithm does not have to verify that the pivot is in the middle half - if we hit it any constant fraction of the times, that is enough for the desired complexity.
The outline of a formal proof of the O(nlogn) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. Choosing a pivot, uniformly at random from 0 to n − 1, is then equivalent to choosing the size of one particular partition, uniformly at random from 0 to n − 1. With this observation, the continuation of the proof is analogous to the one given in the average complexity section.
[edit]Average complexity
Even if pivots aren't chosen randomly, quicksort still requires only  time over all possible permutations of its input. Because this average is simply the sum of the times over all permutations of the input divided by n factorial, it's equivalent to choosing a random permutation of the input. When we do this, the pivot choices are essentially random, leading to an algorithm with the same running time as randomized quicksort.
More precisely, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:

Here, n − 1 is the number of comparisons the partition uses. Since the pivot is equally likely to fall anywhere in the sorted list order, the sum is averaging over all possible splits.
This means that, on average, quicksort performs only about 39% worse than the ideal number of comparisons, which is its best case. In this sense it is closer to the best case than the worst case. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.
[edit]Space complexity
The space used by quicksort depends on the version used.
Quicksort has a space complexity of , even in the worst case, when it is carefully implemented ensuring the following two properties:
in-place partitioning is used. This requires .
After partitioning, the partition with the fewest elements is (recursively) sorted first, requiring at most  space. Then the other partition is sorted using tail recursion or iteration, which doesn't add to the call stack. This idea, as discussed above, was described by R. Sedgewick.[3][4]
The version of quicksort with in-place partitioning uses only constant additional space before making any recursive call. However, if it has made  nested recursive calls, it needs to store a constant amount of information from each of them. Since the best case makes at most  nested recursive calls, it uses  space. The worst case makes  nested recursive calls, and so needs  space; Sedgewick's improved version using tail recursion requires  space in the worst case.
We are eliding a small detail here, however. If we consider sorting arbitrarily large lists, we have to keep in mind that our variables like left and right can no longer be considered to occupy constant space; it takes  bits to index into a list of n items. Because we have variables like this in every stack frame, in reality quicksort requires  bits of space in the best and average case and  space in the worst case. This isn't too terrible, though, since if the list contains mostly distinct elements, the list itself will also occupy  bits of space.
The not-in-place version of quicksort uses  space before it even makes any recursive calls. In the best case its space is still limited to , because each level of the recursion uses half as much space as the last, and

Its worst case is dismal, requiring

space, far more than the list itself. If the list elements are not themselves constant size, the problem grows even larger; for example, if most of the list elements are distinct, each would require about  bits, leading to a best-case and worst-case  space requirement.
[edit]Selection-based pivoting

A selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, except that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist which contains the desired element. This small change lowers the average complexity to linear or  time, and makes it an in-place algorithm. A variation on this algorithm brings the worst-case time down to  (see selection algorithm for more information).
Conversely, once we know a worst-case  selection algorithm is available, we can use it to find the ideal pivot (the median) at every step of quicksort, producing a variant with worst-case  running time. In practical implementations, however, this variant is considerably slower on average.
Another variant is to choose the Median of Medians as the pivot element instead of the median itself for partitioning the elements. While maintaining the asymptotically optimal run time complexity of  (by preventing worst case partitions), it is also considerably faster than the variant that chooses the median as pivot.
[edit]Variants

There are three well known variants of quicksort:
Balanced quicksort: choose a pivot likely to represent the middle of the values to be sorted, and then follow the regular quicksort algorithm.
External quicksort: The same as regular quicksort except the pivot is replaced by a buffer. First, read the M/2 first and last elements into the buffer and sort them. Read the next element from the beginning or end to balance writing. If the next element is less than the least of the buffer, write it to available space at the beginning. If greater than the greatest, write it to the end. Otherwise write the greatest or least of the buffer, and put the next element in the buffer. Keep the maximum lower and minimum upper keys written to avoid resorting middle elements that are in order. When done, write the buffer. Recursively sort the smaller partition, and loop to sort the remaining partition.
Three-way radix quicksort (also called multikey quicksort): is a combination of radix sort and quicksort. Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey). Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character. Recursively sort the "less than" and "greater than" partitions on the same character. Recursively sort the "equal to" partition by the next character (key).
[edit]Comparison with other sorting algorithms

Quicksort is a space-optimized version of the binary tree sort. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order.
The most direct competitor of quicksort is heapsort. Heapsort's worst-case running time is always . But, heapsort is assumed to be on average somewhat slower than quicksort. This is still debated and in research, with some publications indicating the opposite.[11][12] In Quicksort remains the chance of worst case performance except in the introsort variant, which switches to heapsort when a bad case is detected. If it is known in advance that heapsort is going to be necessary, using it directly will be faster than waiting for introsort to switch to it.
Quicksort also competes with mergesort, another recursive sort algorithm but with the benefit of worst-case  running time. Mergesort is a stable sort, unlike quicksort and heapsort, and can be easily adapted to operate on linked lists and very large lists stored on slow-to-access media such as disk storage or network attached storage. Although quicksort can be written to operate on linked lists, it will often suffer from poor pivot choices without random access. The main disadvantage of mergesort is that, when operating on arrays, it requires  auxiliary space in the best case, whereas the variant of quicksort with in-place partitioning and tail recursion uses only  space. (Note that when operating on linked lists, mergesort only requires a small, constant amount of auxiliary storage.)
Bucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.
